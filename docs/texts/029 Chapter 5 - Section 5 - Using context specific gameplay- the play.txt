Chapter 5 - Section 5.: Using context specific gameplay: the play

   It was at this point, with some context specific gameplay in hand that I started to run through a few scenarios with James, my XO and my Chief Scientist in our boardroom. Our plan started to coalesce and was enhanced by various experiments that the company had conducted. Not least of which was the head of my frameworks team walking in to tell me that they had just demonstrated we could develop entire applications (front end and back end) in Javascript.

   At the same time as refining our play, I had encouraged the group to develop component services under the moniker of LibApi as in liberation API i.e. our freedom from endlessly repeated tasks and our existing business model. To say I was rapturous by this experiment would be to underestimate my pure delight. This fortuitous event helped cement the plan which is summarised in figure 57. I’ll break it down and go through each point in detail.
See    Figure 57: The Plan
 for more details.
   Point 1 — the focus of the company would be on providing a code execution platform as a utility service alongside an expanding range of industrialised component services for common tasks such as billing, messaging, an object store (a key-object store API), email etc. All components would be exposed through public APIs and the service would provide the ability to develop entire applications in a single language — JavaScript. The choice of JavaScript was because of its common use, the security of the JS engine and the removal of translation errors with both the front and back end code built in the same language. The entire environment would be charged on the basis of JavaScript operations, network usage and storage. There would be no concept of a physical or virtual machine.

   Point 2 — to accelerate the development of the platform, the entire service would be open sourced. This would also enable other companies to set up competing services but this was planned for and desirable.

   Point 3 — the goal was not to create one Zimki service (the name given to our platform) but instead a competitive marketplace of providers. We were aiming to grab a small but lucrative piece of a very large pie by seeding the market with our own utility service and then open sourcing the technology. To prevent companies from creating different product versions the entire system needed to be open sourced under a license which enabled competition on an operational level but minimised feature differentiation of a product set — GPL seemed to fit the bill.

   We still had a problem that service providers could differentiate and undermine the market. However, we also had a solution as our development process used test driven development and the entire platform was exposed through APIs. In the process of developing we had created an extensive testing suite. This testing suite would be used to distinguish between community platforms providers (those who have taken the code but modified it in a significant way) and certified Zimki providers (those who complied with the testing suite). Through the use of a trademarked image for Zimki providers we could enforce some level of portability between the providers.

   By creating this marketplace, backed by an Open Zimki Foundation, we could overcome one source of inertia (reliance on a single provider) whilst enabling companies to try their own platform in-house first and developing new opportunities for ourselves from an application store, market reporting, switching services, brokerage capability, training, support and pre built stand-alone Zimki clusters. Such an approach would also reduce our capital exposure given the constraints we existed under.

   Point 4 — we needed to build an ecosystem to allow us to identify the future services we should create and hence we had to build an ILC model. Obviously we could only directly observe the consumption data for those who built on our service but what about other Zimki providers?

   By providing common services such as GUBE (generic utility billing engine) along with an application store, a component library (a CPAN equivalent) and ultimately some form of brokerage capability then we intended to create multiple sources of meta data. We had a lot of discussion here over whether we could go it alone but I felt we didn’t have the brand name. We needed to create that marketplace and the potential was huge. I had estimated that the entire utility computing market (i.e. cloud computing) would be worth $200bn a decade later in 2016 and we would grab small piece.

   Our longer term prize was to be the market enabler and ultimately build some form of financial exchange. We would require outside help to make this happen given our constraints but we decided not to promote that message as it was “too far in the future and too crazy” for most.

   Point 5 — we needed to make it easy, quick and cheap for people to build entire applications on our platform. We had to ruthlessly cut away all the yak shaving (pointless, unpleasant and repeated tasks) that were involved in developing. When one of the development team built an entirely new form of wiki with client side preview and went from idea to launching live on the web in an under an hour then I knew we had something with potential. Pre-shaved Yaks became the catch-phrase to describe the service and something we plastered across our T-Shirts in 2005 and 2006.

   Point 6 — we anticipated that someone would provide a utility infrastructure service. We needed to exploit this by building on top of them. We had become pretty handy at building worth based services (i.e. ones we charged for on a percentage of the value they created) over the years and I knew we could balance our charging of the platform against any variable operational cost caused by a utility infrastructure provider.

   By building on top of any utility infrastructure service, we would also have the advantage of cutting that supplier off from any meta data other than our platform was growing. If I played the game well enough then maybe that would be an exit play for us through acquisition. If we were truly going to be successful, then I would need to break the anchor of the parent company at some point in the future.
   Point 7 — we knew that building data centres would be a constraint in utility infrastructure and that compute demand was elastic. This gave options for counter play such as creating a price war to force up the demand beyond the ability of one supplier to provide. But in order to play one provider off against another we needed to give competitors a route into the market. Fortunately, we had our Borg system and though we had talked with one large well known hardware provider (who had been resistant to the idea of utility compute) we could open source (Point 8) this space to encourage that market to form. I had counter plays I could use if needed them and it was to our advantage if a fragmented market of utility infrastructure providers existed. We should aim for no-one company to gain overall control of this space.

   The option looked good based upon our capabilities. It was within the realm of possibilities and mindful of the constraints we had. This seemed to provide the best path forward. It would mean refocusing the company, removing services like our online photo site and putting other revenue services into some form of minimal state until the platform business grew enough that we could dispose of them. I was ready to pull the trigger but there was one last thing I needed.

